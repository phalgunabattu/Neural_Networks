{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_for_image classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEYNx3cIGhGx"
      },
      "source": [
        "# Using CNNs to Classify Hand-written Digits on MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYLFQGh3KQe4",
        "outputId": "d444fcb1-5172-4fd8-865c-4b6f9b202b07"
      },
      "source": [
        "# keras imports for the dataset and building our neural network\r\n",
        "\r\n",
        "#The below model in this cell is ANN which is just coded to give the brief explanation of how ANN deals with image data\r\n",
        "\r\n",
        "from keras.datasets import mnist\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D\r\n",
        "from keras.utils import np_utils\r\n",
        "\r\n",
        "\r\n",
        "# Flattening the images from the 28x28 pixels to 1D 784 pixels\r\n",
        "X_train = X_train.reshape(60000, 784)\r\n",
        "X_test = X_test.reshape(10000, 784)\r\n",
        "X_train = X_train.astype('float32')\r\n",
        "X_test = X_test.astype('float32')\r\n",
        "\r\n",
        "# normalizing the data to help with the training\r\n",
        "X_train /= 255\r\n",
        "X_test /= 255\r\n",
        "\r\n",
        "# one-hot encoding using keras' numpy-related utilities\r\n",
        "n_classes = 10\r\n",
        "print(\"Shape before one-hot encoding: \", y_train.shape)\r\n",
        "Y_train = np_utils.to_categorical(y_train, n_classes)\r\n",
        "Y_test = np_utils.to_categorical(y_test, n_classes)\r\n",
        "print(\"Shape after one-hot encoding: \", Y_train.shape)\r\n",
        "\r\n",
        "# building a linear stack of layers with the sequential model\r\n",
        "model = Sequential()\r\n",
        "# hidden layer\r\n",
        "model.add(Dense(100, input_shape=(784,), activation='relu'))\r\n",
        "# output layer\r\n",
        "model.add(Dense(10, activation='softmax'))\r\n",
        "\r\n",
        "# looking at the model summary\r\n",
        "model.summary()\r\n",
        "# compiling the sequential model\r\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\r\n",
        "# training the model for 10 epochs\r\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape before one-hot encoding:  (60000,)\n",
            "Shape after one-hot encoding:  (60000, 10)\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_11 (Dense)             (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 79,510\n",
            "Trainable params: 79,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 2.1211 - accuracy: 0.4600 - val_loss: 1.2562 - val_accuracy: 0.7456\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 1.0950 - accuracy: 0.7614 - val_loss: 0.7368 - val_accuracy: 0.8310\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.8378 - val_loss: 0.5441 - val_accuracy: 0.8658\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.5285 - accuracy: 0.8698 - val_loss: 0.4507 - val_accuracy: 0.8854\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.4526 - accuracy: 0.8817 - val_loss: 0.3976 - val_accuracy: 0.8959\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3934 - accuracy: 0.8953 - val_loss: 0.3655 - val_accuracy: 0.9003\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3704 - accuracy: 0.8974 - val_loss: 0.3445 - val_accuracy: 0.9052\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3510 - accuracy: 0.9009 - val_loss: 0.3276 - val_accuracy: 0.9088\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3344 - accuracy: 0.9054 - val_loss: 0.3162 - val_accuracy: 0.9112\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.3214 - accuracy: 0.9096 - val_loss: 0.3052 - val_accuracy: 0.9130\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb6aed075c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT-1sLqiKuNg"
      },
      "source": [
        "# Lets bring in convolutions\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJOnHqJIGiDw",
        "outputId": "a815c03b-e3ee-4935-d6ef-12f99e50a8b0"
      },
      "source": [
        "# Now we will deep dive to CNN and see how it deals with image data\r\n",
        "from keras.datasets import mnist\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\r\n",
        "from keras.utils import np_utils\r\n",
        "from keras.datasets import mnist\r\n",
        "\r\n",
        "# loading the dataset\r\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\r\n",
        "\r\n",
        "# let's print the shape of the dataset\r\n",
        "print(\"X_train shape\", X_train.shape)\r\n",
        "print(\"y_train shape\", y_train.shape)\r\n",
        "print(\"X_test shape\", X_test.shape)\r\n",
        "print(\"y_test shape\", y_test.shape)\r\n",
        "\r\n",
        "'''\r\n",
        "Flatten the input image dimensions to 1D (width pixels x height pixels)\r\n",
        "Normalize the image pixel values (divide by 255)\r\n",
        "One-Hot Encode the categorical column\r\n",
        "Build a model architecture (Sequential) with Dense layers\r\n",
        "Train the model and make predictions'''\r\n",
        "\r\n",
        "# 1. Flattening the images from the 28x28 pixels to 1D 784 pixels\r\n",
        "X_train = X_train.reshape(60000, 28,28,1)\r\n",
        "X_test = X_test.reshape(10000, 28,28,1)\r\n",
        "X_train = X_train.astype('float32')\r\n",
        "X_test = X_test.astype('float32')\r\n",
        "\r\n",
        "# 2. normalizing the data to help with the training\r\n",
        "X_train /= 255\r\n",
        "X_test /= 255\r\n",
        "\r\n",
        "# 3. one-hot encoding using keras' numpy-related utilities\r\n",
        "n_classes = 10\r\n",
        "print(\"Shape before one-hot encoding: \", y_train.shape)\r\n",
        "Y_train = np_utils.to_categorical(y_train, n_classes)\r\n",
        "Y_test = np_utils.to_categorical(y_test, n_classes)\r\n",
        "print(\"Shape after one-hot encoding: \", Y_train.shape)\r\n",
        "\r\n",
        "# 4. building a linear stack of layers with the sequential model\r\n",
        "model = Sequential()\r\n",
        "# convolutional layer\r\n",
        "model.add(Conv2D(25, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu', input_shape=(28,28,1)))\r\n",
        "model.add(MaxPool2D(pool_size=(1,1)))\r\n",
        "# flatten output of conv\r\n",
        "model.add(Flatten())\r\n",
        "# hidden layer\r\n",
        "model.add(Dense(100, activation='relu'))\r\n",
        "# output layer\r\n",
        "model.add(Dense(10, activation='softmax'))\r\n",
        "\r\n",
        "# compiling the sequential model\r\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\r\n",
        "\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "# 5. training the model for 10 epochs\r\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape (60000, 28, 28)\n",
            "y_train shape (60000,)\n",
            "X_test shape (10000, 28, 28)\n",
            "y_test shape (10000,)\n",
            "Shape before one-hot encoding:  (60000,)\n",
            "Shape after one-hot encoding:  (60000, 10)\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 26, 26, 25)        250       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 26, 26, 25)        0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 16900)             0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 100)               1690100   \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 1,691,360\n",
            "Trainable params: 1,691,360\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3715 - accuracy: 0.8958 - val_loss: 0.0730 - val_accuracy: 0.9770\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0631 - accuracy: 0.9816 - val_loss: 0.0519 - val_accuracy: 0.9811\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0328 - accuracy: 0.9906 - val_loss: 0.0509 - val_accuracy: 0.9836\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0195 - accuracy: 0.9944 - val_loss: 0.0459 - val_accuracy: 0.9848\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0119 - accuracy: 0.9971 - val_loss: 0.0496 - val_accuracy: 0.9838\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0089 - accuracy: 0.9977 - val_loss: 0.0484 - val_accuracy: 0.9855\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0053 - accuracy: 0.9987 - val_loss: 0.0479 - val_accuracy: 0.9859\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.0511 - val_accuracy: 0.9856\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0077 - accuracy: 0.9978 - val_loss: 0.0596 - val_accuracy: 0.9848\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0574 - val_accuracy: 0.9860\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb6aff470f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYn6YjGBK2D6"
      },
      "source": [
        "# Identifying Images from the CIFAR-10 Dataset using CNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTNHan9CIW8g",
        "outputId": "ec338c92-b95c-4564-91aa-69edcd3bd983"
      },
      "source": [
        "# keras imports for the dataset and building our neural network\r\n",
        "from keras.datasets import cifar10\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\r\n",
        "from keras.utils import np_utils\r\n",
        "\r\n",
        "# loading the dataset\r\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\r\n",
        "\r\n",
        "# # building the input vector from the 32x32 pixels\r\n",
        "X_train = X_train.reshape(X_train.shape[0], 32, 32, 3)\r\n",
        "X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\r\n",
        "X_train = X_train.astype('float32')\r\n",
        "X_test = X_test.astype('float32')\r\n",
        "\r\n",
        "# normalizing the data to help with the training\r\n",
        "X_train /= 255\r\n",
        "X_test /= 255\r\n",
        "\r\n",
        "# one-hot encoding using keras' numpy-related utilities\r\n",
        "n_classes = 10\r\n",
        "print(\"Shape before one-hot encoding: \", y_train.shape)\r\n",
        "Y_train = np_utils.to_categorical(y_train, n_classes)\r\n",
        "Y_test = np_utils.to_categorical(y_test, n_classes)\r\n",
        "print(\"Shape after one-hot encoding: \", Y_train.shape)\r\n",
        "\r\n",
        "# building a linear stack of layers with the sequential model\r\n",
        "model = Sequential()\r\n",
        "\r\n",
        "# convolutional layer\r\n",
        "model.add(Conv2D(50, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', input_shape=(32, 32, 3)))\r\n",
        "\r\n",
        "# convolutional layer\r\n",
        "model.add(Conv2D(75, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\r\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "\r\n",
        "model.add(Conv2D(125, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\r\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "\r\n",
        "# flatten output of conv\r\n",
        "model.add(Flatten())\r\n",
        "\r\n",
        "# hidden layer\r\n",
        "model.add(Dense(500, activation='relu'))\r\n",
        "model.add(Dropout(0.4))\r\n",
        "model.add(Dense(250, activation='relu'))\r\n",
        "model.add(Dropout(0.3))\r\n",
        "# output layer\r\n",
        "model.add(Dense(10, activation='softmax'))\r\n",
        "\r\n",
        "# compiling the sequential model\r\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\r\n",
        "\r\n",
        "# training the model for 10 epochs\r\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "Shape before one-hot encoding:  (50000, 1)\n",
            "Shape after one-hot encoding:  (50000, 10)\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 14s 19ms/step - loss: 1.8720 - accuracy: 0.2963 - val_loss: 1.1767 - val_accuracy: 0.5866\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 7s 18ms/step - loss: 1.1805 - accuracy: 0.5790 - val_loss: 0.9187 - val_accuracy: 0.6777\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 7s 19ms/step - loss: 0.9432 - accuracy: 0.6728 - val_loss: 0.8168 - val_accuracy: 0.7182\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 7s 19ms/step - loss: 0.8264 - accuracy: 0.7091 - val_loss: 0.7618 - val_accuracy: 0.7375\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 7s 19ms/step - loss: 0.7396 - accuracy: 0.7437 - val_loss: 0.7109 - val_accuracy: 0.7542\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 7s 19ms/step - loss: 0.6662 - accuracy: 0.7679 - val_loss: 0.7024 - val_accuracy: 0.7539\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 7s 18ms/step - loss: 0.6060 - accuracy: 0.7883 - val_loss: 0.6782 - val_accuracy: 0.7679\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 7s 18ms/step - loss: 0.5668 - accuracy: 0.8023 - val_loss: 0.7084 - val_accuracy: 0.7610\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 7s 19ms/step - loss: 0.5171 - accuracy: 0.8159 - val_loss: 0.6490 - val_accuracy: 0.7767\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 7s 19ms/step - loss: 0.4847 - accuracy: 0.8300 - val_loss: 0.6635 - val_accuracy: 0.7746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb6a61b37f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiHFqhJQK8E_"
      },
      "source": [
        "# Categorizing the Images of ImageNet using CNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58a9kI9-Iv3w",
        "outputId": "3d6ffbe3-2923-4ebd-ebeb-f4d5da257e94"
      },
      "source": [
        "# downloading the IMAGENET data\r\n",
        "! wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz\r\n",
        "! tar -xf imagenette2.tgz"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-20 06:41:45--  https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.82.67\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.82.67|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1557161267 (1.5G) [application/x-tar]\n",
            "Saving to: ‘imagenette2.tgz’\n",
            "\n",
            "imagenette2.tgz     100%[===================>]   1.45G  45.7MB/s    in 32s     \n",
            "\n",
            "2021-02-20 06:42:18 (46.0 MB/s) - ‘imagenette2.tgz’ saved [1557161267/1557161267]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9193Y4jK_y4"
      },
      "source": [
        "imagenette_map = { \r\n",
        "    \"n01440764\" : \"tench\",\r\n",
        "    \"n02102040\" : \"springer\",\r\n",
        "    \"n02979186\" : \"casette_player\",\r\n",
        "    \"n03000684\" : \"chain_saw\",\r\n",
        "    \"n03028079\" : \"church\",\r\n",
        "    \"n03394916\" : \"French_horn\",\r\n",
        "    \"n03417042\" : \"garbage_truck\",\r\n",
        "    \"n03425413\" : \"gas_pump\",\r\n",
        "    \"n03445777\" : \"golf_ball\",\r\n",
        "    \"n03888257\" : \"parachute\"\r\n",
        "}"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YT3jV3WtLSO_",
        "outputId": "1e8e9055-4590-429d-f0e4-0d8e0fd401b2"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "\r\n",
        "# create a new generator\r\n",
        "imagegen = ImageDataGenerator()\r\n",
        "# load train data\r\n",
        "train = imagegen.flow_from_directory(\"imagenette2/train/\", class_mode=\"categorical\", shuffle=False, batch_size=128, target_size=(224, 224))\r\n",
        "# load val data\r\n",
        "val = imagegen.flow_from_directory(\"imagenette2/val/\", class_mode=\"categorical\", shuffle=False, batch_size=128, target_size=(224, 224))\r\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9469 images belonging to 10 classes.\n",
            "Found 3925 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtwYKuwLLXCo",
        "outputId": "d762a7aa-b418-4ef8-c2bf-cce51ff2908b"
      },
      "source": [
        "# Building a Basic CNN model for Image Classification\r\n",
        "\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, InputLayer, BatchNormalization, Dropout\r\n",
        "\r\n",
        "# build a sequential model\r\n",
        "model = Sequential()\r\n",
        "model.add(InputLayer(input_shape=(224, 224, 3)))\r\n",
        "\r\n",
        "# 1st conv block\r\n",
        "model.add(Conv2D(25, (5, 5), activation='relu', strides=(1, 1), padding='same'))\r\n",
        "model.add(MaxPool2D(pool_size=(2, 2), padding='same'))\r\n",
        "# 2nd conv block\r\n",
        "model.add(Conv2D(50, (5, 5), activation='relu', strides=(2, 2), padding='same'))\r\n",
        "model.add(MaxPool2D(pool_size=(2, 2), padding='same'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "# 3rd conv block\r\n",
        "model.add(Conv2D(70, (3, 3), activation='relu', strides=(2, 2), padding='same'))\r\n",
        "model.add(MaxPool2D(pool_size=(2, 2), padding='valid'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "# ANN block\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(units=100, activation='relu'))\r\n",
        "model.add(Dense(units=100, activation='relu'))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "# output layer\r\n",
        "model.add(Dense(units=10, activation='softmax'))\r\n",
        "\r\n",
        "# compile model\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\r\n",
        "# fit on data for 30 epochs\r\n",
        "model.fit_generator(train, epochs=30, validation_data=val)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "74/74 [==============================] - 69s 926ms/step - loss: 2.7410 - accuracy: 0.1270 - val_loss: 2.3126 - val_accuracy: 0.1541\n",
            "Epoch 2/30\n",
            "74/74 [==============================] - 67s 911ms/step - loss: 2.0104 - accuracy: 0.3198 - val_loss: 2.1843 - val_accuracy: 0.2484\n",
            "Epoch 3/30\n",
            "74/74 [==============================] - 67s 909ms/step - loss: 1.9036 - accuracy: 0.3642 - val_loss: 1.8767 - val_accuracy: 0.3659\n",
            "Epoch 4/30\n",
            "74/74 [==============================] - 67s 912ms/step - loss: 1.5912 - accuracy: 0.4639 - val_loss: 1.9496 - val_accuracy: 0.3343\n",
            "Epoch 5/30\n",
            "74/74 [==============================] - 67s 913ms/step - loss: 1.3229 - accuracy: 0.5567 - val_loss: 2.1618 - val_accuracy: 0.3422\n",
            "Epoch 6/30\n",
            "74/74 [==============================] - 68s 915ms/step - loss: 1.2874 - accuracy: 0.5674 - val_loss: 2.0740 - val_accuracy: 0.3638\n",
            "Epoch 7/30\n",
            "74/74 [==============================] - 67s 912ms/step - loss: 0.9902 - accuracy: 0.6685 - val_loss: 2.0381 - val_accuracy: 0.3781\n",
            "Epoch 8/30\n",
            "74/74 [==============================] - 67s 902ms/step - loss: 0.8481 - accuracy: 0.7128 - val_loss: 2.0713 - val_accuracy: 0.4316\n",
            "Epoch 9/30\n",
            "74/74 [==============================] - 67s 906ms/step - loss: 0.5787 - accuracy: 0.8074 - val_loss: 1.9877 - val_accuracy: 0.4148\n",
            "Epoch 10/30\n",
            "74/74 [==============================] - 67s 905ms/step - loss: 0.4457 - accuracy: 0.8532 - val_loss: 2.0106 - val_accuracy: 0.4464\n",
            "Epoch 11/30\n",
            "74/74 [==============================] - 67s 911ms/step - loss: 0.5126 - accuracy: 0.8264 - val_loss: 1.9575 - val_accuracy: 0.4489\n",
            "Epoch 12/30\n",
            "74/74 [==============================] - 67s 906ms/step - loss: 0.3201 - accuracy: 0.8997 - val_loss: 2.2734 - val_accuracy: 0.4181\n",
            "Epoch 13/30\n",
            "74/74 [==============================] - 67s 907ms/step - loss: 0.2104 - accuracy: 0.9381 - val_loss: 2.2894 - val_accuracy: 0.4405\n",
            "Epoch 14/30\n",
            "74/74 [==============================] - 67s 897ms/step - loss: 0.1359 - accuracy: 0.9618 - val_loss: 2.4324 - val_accuracy: 0.4405\n",
            "Epoch 15/30\n",
            "74/74 [==============================] - 67s 903ms/step - loss: 0.1084 - accuracy: 0.9700 - val_loss: 2.1478 - val_accuracy: 0.4685\n",
            "Epoch 16/30\n",
            "74/74 [==============================] - 66s 899ms/step - loss: 0.0879 - accuracy: 0.9752 - val_loss: 2.6692 - val_accuracy: 0.4530\n",
            "Epoch 17/30\n",
            "74/74 [==============================] - 67s 903ms/step - loss: 0.0976 - accuracy: 0.9709 - val_loss: 3.1575 - val_accuracy: 0.3850\n",
            "Epoch 18/30\n",
            "74/74 [==============================] - 67s 901ms/step - loss: 0.0718 - accuracy: 0.9786 - val_loss: 2.6324 - val_accuracy: 0.4392\n",
            "Epoch 19/30\n",
            "74/74 [==============================] - 66s 897ms/step - loss: 0.0334 - accuracy: 0.9931 - val_loss: 2.6122 - val_accuracy: 0.4573\n",
            "Epoch 20/30\n",
            "74/74 [==============================] - 67s 901ms/step - loss: 0.0272 - accuracy: 0.9934 - val_loss: 3.1422 - val_accuracy: 0.4222\n",
            "Epoch 21/30\n",
            "74/74 [==============================] - 66s 894ms/step - loss: 0.0275 - accuracy: 0.9939 - val_loss: 2.9666 - val_accuracy: 0.4321\n",
            "Epoch 22/30\n",
            "74/74 [==============================] - 66s 892ms/step - loss: 0.0190 - accuracy: 0.9964 - val_loss: 3.2749 - val_accuracy: 0.4375\n",
            "Epoch 23/30\n",
            "74/74 [==============================] - 66s 897ms/step - loss: 0.0284 - accuracy: 0.9917 - val_loss: 3.0195 - val_accuracy: 0.4352\n",
            "Epoch 24/30\n",
            "74/74 [==============================] - 66s 897ms/step - loss: 0.0194 - accuracy: 0.9962 - val_loss: 3.4368 - val_accuracy: 0.4110\n",
            "Epoch 25/30\n",
            "74/74 [==============================] - 66s 893ms/step - loss: 0.0349 - accuracy: 0.9896 - val_loss: 4.2957 - val_accuracy: 0.3778\n",
            "Epoch 26/30\n",
            "74/74 [==============================] - 66s 891ms/step - loss: 0.1032 - accuracy: 0.9688 - val_loss: 4.8337 - val_accuracy: 0.3292\n",
            "Epoch 27/30\n",
            "74/74 [==============================] - 66s 888ms/step - loss: 0.6454 - accuracy: 0.8137 - val_loss: 3.4839 - val_accuracy: 0.3679\n",
            "Epoch 28/30\n",
            "74/74 [==============================] - 66s 895ms/step - loss: 0.2444 - accuracy: 0.9209 - val_loss: 2.8185 - val_accuracy: 0.4461\n",
            "Epoch 29/30\n",
            "74/74 [==============================] - 67s 904ms/step - loss: 0.0520 - accuracy: 0.9872 - val_loss: 3.7396 - val_accuracy: 0.4229\n",
            "Epoch 30/30\n",
            "74/74 [==============================] - 67s 902ms/step - loss: 0.0209 - accuracy: 0.9964 - val_loss: 2.8489 - val_accuracy: 0.4611\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb6ae93cbe0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CCGfTxjLkS6"
      },
      "source": [
        "# Using Transfer Learning (VGG16) to improve accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU7aJp1PLc2I",
        "outputId": "c89619a2-462f-43de-bb55-bf366771209f"
      },
      "source": [
        "# Downloading weights of VGG16\r\n",
        "from keras.applications import VGG16\r\n",
        "# include top should be False to remove the softmax layer\r\n",
        "pretrained_model = VGG16(include_top=False, weights='imagenet')\r\n",
        "pretrained_model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, None, None, 3)]   0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS6z9wvnL6dA",
        "outputId": "9ae7ff52-80b1-4d2a-cf24-fac839c00f5e"
      },
      "source": [
        "# Generate features from VGG16\r\n",
        "from keras.utils import to_categorical\r\n",
        "# extract train and val features\r\n",
        "vgg_features_train = pretrained_model.predict(train)\r\n",
        "vgg_features_val = pretrained_model.predict(val)\r\n",
        "\r\n",
        "# OHE target column\r\n",
        "train_target = to_categorical(train.labels)\r\n",
        "val_target = to_categorical(val.labels)\r\n",
        "\r\n",
        "# Once the above features are ready, we can just use them to train a basic Fully Connected Neural Network in Keras\r\n",
        "model2 = Sequential()\r\n",
        "model2.add(Flatten(input_shape=(7,7,512)))\r\n",
        "model2.add(Dense(100, activation='relu'))\r\n",
        "model2.add(Dropout(0.5))\r\n",
        "model2.add(BatchNormalization())\r\n",
        "model2.add(Dense(10, activation='softmax'))\r\n",
        "\r\n",
        "# compile the model\r\n",
        "model2.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy')\r\n",
        "\r\n",
        "model2.summary()\r\n",
        "\r\n",
        "# train model using features generated from VGG16 model\r\n",
        "model2.fit(vgg_features_train, train_target, epochs=50, batch_size=128, validation_data=(vgg_features_val, val_target))\r\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_6 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 100)               2508900   \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 2,510,310\n",
            "Trainable params: 2,510,110\n",
            "Non-trainable params: 200\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "74/74 [==============================] - 1s 13ms/step - loss: 0.7231 - accuracy: 0.7942 - val_loss: 0.1981 - val_accuracy: 0.9376\n",
            "Epoch 2/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.1484 - accuracy: 0.9705 - val_loss: 0.1754 - val_accuracy: 0.9455\n",
            "Epoch 3/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0669 - accuracy: 0.9919 - val_loss: 0.1734 - val_accuracy: 0.9465\n",
            "Epoch 4/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0375 - accuracy: 0.9974 - val_loss: 0.1664 - val_accuracy: 0.9488\n",
            "Epoch 5/50\n",
            "74/74 [==============================] - 1s 12ms/step - loss: 0.0240 - accuracy: 0.9985 - val_loss: 0.1703 - val_accuracy: 0.9475\n",
            "Epoch 6/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0176 - accuracy: 0.9989 - val_loss: 0.1693 - val_accuracy: 0.9470\n",
            "Epoch 7/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0139 - accuracy: 0.9994 - val_loss: 0.1729 - val_accuracy: 0.9457\n",
            "Epoch 8/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0124 - accuracy: 0.9992 - val_loss: 0.1751 - val_accuracy: 0.9478\n",
            "Epoch 9/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0098 - accuracy: 0.9995 - val_loss: 0.1730 - val_accuracy: 0.9473\n",
            "Epoch 10/50\n",
            "74/74 [==============================] - 1s 12ms/step - loss: 0.0074 - accuracy: 0.9997 - val_loss: 0.1712 - val_accuracy: 0.9475\n",
            "Epoch 11/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0061 - accuracy: 0.9998 - val_loss: 0.1759 - val_accuracy: 0.9490\n",
            "Epoch 12/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0055 - accuracy: 0.9999 - val_loss: 0.1845 - val_accuracy: 0.9478\n",
            "Epoch 13/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0057 - accuracy: 0.9997 - val_loss: 0.1855 - val_accuracy: 0.9478\n",
            "Epoch 14/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.1789 - val_accuracy: 0.9496\n",
            "Epoch 15/50\n",
            "74/74 [==============================] - 1s 12ms/step - loss: 0.0043 - accuracy: 0.9999 - val_loss: 0.1816 - val_accuracy: 0.9493\n",
            "Epoch 16/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0039 - accuracy: 0.9999 - val_loss: 0.1873 - val_accuracy: 0.9493\n",
            "Epoch 17/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0036 - accuracy: 0.9999 - val_loss: 0.1858 - val_accuracy: 0.9501\n",
            "Epoch 18/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.1893 - val_accuracy: 0.9496\n",
            "Epoch 19/50\n",
            "74/74 [==============================] - 1s 12ms/step - loss: 0.0031 - accuracy: 0.9999 - val_loss: 0.2013 - val_accuracy: 0.9470\n",
            "Epoch 20/50\n",
            "74/74 [==============================] - 1s 12ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.1937 - val_accuracy: 0.9457\n",
            "Epoch 21/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.2031 - val_accuracy: 0.9460\n",
            "Epoch 22/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.1934 - val_accuracy: 0.9445\n",
            "Epoch 23/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0031 - accuracy: 0.9999 - val_loss: 0.1967 - val_accuracy: 0.9450\n",
            "Epoch 24/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0029 - accuracy: 0.9998 - val_loss: 0.2055 - val_accuracy: 0.9434\n",
            "Epoch 25/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.2084 - val_accuracy: 0.9419\n",
            "Epoch 26/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2077 - val_accuracy: 0.9432\n",
            "Epoch 27/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 0.2026 - val_accuracy: 0.9439\n",
            "Epoch 28/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0030 - accuracy: 0.9999 - val_loss: 0.2142 - val_accuracy: 0.9411\n",
            "Epoch 29/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0031 - accuracy: 0.9999 - val_loss: 0.2197 - val_accuracy: 0.9404\n",
            "Epoch 30/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 0.2117 - val_accuracy: 0.9424\n",
            "Epoch 31/50\n",
            "74/74 [==============================] - 1s 12ms/step - loss: 0.0034 - accuracy: 0.9996 - val_loss: 0.2120 - val_accuracy: 0.9470\n",
            "Epoch 32/50\n",
            "74/74 [==============================] - 1s 12ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.2273 - val_accuracy: 0.9442\n",
            "Epoch 33/50\n",
            "74/74 [==============================] - 1s 12ms/step - loss: 0.0045 - accuracy: 0.9991 - val_loss: 0.2347 - val_accuracy: 0.9371\n",
            "Epoch 34/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0055 - accuracy: 0.9986 - val_loss: 0.2318 - val_accuracy: 0.9394\n",
            "Epoch 35/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0074 - accuracy: 0.9981 - val_loss: 0.2464 - val_accuracy: 0.9355\n",
            "Epoch 36/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0070 - accuracy: 0.9987 - val_loss: 0.2301 - val_accuracy: 0.9396\n",
            "Epoch 37/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0071 - accuracy: 0.9984 - val_loss: 0.2464 - val_accuracy: 0.9386\n",
            "Epoch 38/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0074 - accuracy: 0.9987 - val_loss: 0.2406 - val_accuracy: 0.9389\n",
            "Epoch 39/50\n",
            "74/74 [==============================] - 1s 12ms/step - loss: 0.0083 - accuracy: 0.9980 - val_loss: 0.2354 - val_accuracy: 0.9404\n",
            "Epoch 40/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0094 - accuracy: 0.9975 - val_loss: 0.2396 - val_accuracy: 0.9376\n",
            "Epoch 41/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0076 - accuracy: 0.9965 - val_loss: 0.2339 - val_accuracy: 0.9401\n",
            "Epoch 42/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0059 - accuracy: 0.9986 - val_loss: 0.2432 - val_accuracy: 0.9394\n",
            "Epoch 43/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0059 - accuracy: 0.9987 - val_loss: 0.2452 - val_accuracy: 0.9401\n",
            "Epoch 44/50\n",
            "74/74 [==============================] - 1s 12ms/step - loss: 0.0053 - accuracy: 0.9988 - val_loss: 0.2451 - val_accuracy: 0.9404\n",
            "Epoch 45/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0051 - accuracy: 0.9992 - val_loss: 0.2519 - val_accuracy: 0.9376\n",
            "Epoch 46/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.2589 - val_accuracy: 0.9371\n",
            "Epoch 47/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.2548 - val_accuracy: 0.9389\n",
            "Epoch 48/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.2552 - val_accuracy: 0.9389\n",
            "Epoch 49/50\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0040 - accuracy: 0.9982 - val_loss: 0.2487 - val_accuracy: 0.9401\n",
            "Epoch 50/50\n",
            "74/74 [==============================] - 1s 12ms/step - loss: 0.0040 - accuracy: 0.9985 - val_loss: 0.2493 - val_accuracy: 0.9406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb6ae61a198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    }
  ]
}